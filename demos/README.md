# End-to-End Platform Use-Case Application Demos

- [Overview](#overview)
- [Model deployment with Streaming](#model-deployment-with-streaming)
- [Natural Language Processing (NLP)](#nlp-demo)
- [Real-Time User Segmentation](#user-segmentation-demo)
- [Smart Stock Trading](#stocks-demo)
- [Stream Enrichment](#stream-enrich-demo)

<a id="overview"></a>
## Overview

The **demos** tutorials directory contains full end-to-end use-case applications (demos) that demonstrate how to use the Iguazio Data Science Platform (**"the platform"**) and related tools to address data science requirements for different industries and implementations.<br>
For information on how to get additional end-to-end platform demos, see the [**welcome.ipynb**](../welcome.ipynb#additional-demos) notebook or main [**README.md**](../README.md#additional-demos) file.

<a id="model-deployment-with-streaming"></a>
## Model Deployment with Streaming

Deploy a model with streaming information. [The demo](model-deployment-with-streaming/0-setup.ipynb) covers the use case of 1<sup>st</sup>-day churn.

The importance of 1<sup>st</sup>-day churn prediction:
- In some segments of the gaming industry, the average 1st day churn is as high as 70%.
- Acquiring new customers is 5x&ndash;25x more expensive than retaining existing ones.
- Reducing churn by just 5% can boost profitability by 75%.
- Improving retention has a 2x&ndash;4x greater impact on growth than acquisition.
- The probability of selling to an existing customer is 60%&ndash;70%, but only 5%&ndash;20% for a prospect.
- Churn rate also informs metrics like customer lifetime value (LTV).

This demo is comprised of several steps:

![Model deployment with streaming Real-time operational Pipeline](../assets/images/model-deployment-with-streaming.png)

While this demo covers the use case of 1<sup>st</sup>-day churn, it is easy to replace the data, related features and training model and reuse the same workflow for different business cases.

<a id="nlp-demo"></a>
## Natural Language Processing (NLP)

The [**nlp**](nlp/nlp-example.ipynb) demo demonstrates natural language processing (NLP): the application processes natural-language textual data &mdash; including spelling correction and sentiment analysis &mdash; and generates a Nuclio serverless function that translates any given text string to another (configurable) language.

- The textual data is collected and processed by using the [TextBlob](https://textblob.readthedocs.io/) Python NLP library. The processing includes spelling correction and sentiment analysis.
- A serverless function that translates text to another language, which is configured in an environment variable, is generated by using the [Nuclio](https://nuclio.io/) framework.

<a id="user-segmentation-demo"></a>
## Real-Time User Segmentation

The [**real-time user segmentation**](slots-stream/real-time-user-segmentation.ipynb) demo demonstrates how to build a stream-event processor on a sliding time window for tagging and untagging users based on programmatic rules of user behavior.
The events are processed by using a Nuclio function.

<a id="stocks-demo"></a>
## Smart Stock Trading

The [**stocks**](stocks/01-gen-demo-data.ipynb) demo demonstrates a smart stock-trading application: 
the application reads stock-exchange data from an internet service into a time-series database (TSDB); uses Twitter to analyze the market sentiment on specific stocks, in real time; and saves the data to a platform NoSQL table that is used for generating reports and analyzing and visualizing the data on a Grafana dashboard.

- The stock data is read from Twitter by using the [TwythonStreamer](https://twython.readthedocs.io/en/latest/usage/streaming_api.html) Python wrapper to the Twitter Streaming API, and saved to TSDB and NoSQL tables in the platform.
- Sentiment analysis is done by using the [TextBlob](https://textblob.readthedocs.io/) Python library for natural language processing (NLP).
- The analyzed data is visualized as graphs on a [Grafana](https://grafana.com/grafana) dashboard, which is created from the Jupyter notebook code.
  The data is read from both the TSDB and NoSQL stock tables.

<a id="stream-enrich-demo"></a>
## Stream Enrichment

The [**stream-enrich**](stream-enrich/stream-enrich.ipynb) demo demonstrates a typical stream-based data-engineering pipeline, which is required in many real-world scenarios: data is streamed from an event streaming engine; the data is enriched, in real time, using data from a NoSQL table; the enriched data is saved to an output data stream and then consumed from this stream.

- Car-owner data is streamed into the platform from a simulated streaming engine by using an event-triggered [Nuclio](https://nuclio.io/) serverless function.
- The data is written (ingested) into an input platform stream by using the the platform's [Streaming Web API](https://www.iguazio.com/docs/latest-release/reference/api-reference/web-apis/streaming-web-api/).
- The input stream data is enriched with additional data, such as the car's color and vendor, and the enriched data is saved to a NoSQL table by using the platform's [NoSQL Web API](https://www.iguazio.com/docs/latest-release/reference/api-reference/web-apis/nosql-web-api/).
- The Nuclio function writes the enriched data to an output platform data stream by using the platform's Streaming Web API.
- The enriched data is read (consumed) from the output stream by using the platform's Streaming Web API.
