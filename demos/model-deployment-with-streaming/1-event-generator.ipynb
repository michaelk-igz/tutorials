{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Event Generator\n",
    "  --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an events stream to simulate incoming data. In real life situations you will provide input data instead of this simulated data.\n",
    "\n",
    "The output is a stream of events called `generated-stream`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model deployment with streaming Real-time operational Pipeline](../../assets/images/model-deployment-with-streaming.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event generator creates the following events: `new_registration`, `new_purchases`, `new_bet` and `new_win` with the following data:\n",
    "\n",
    "| new_registration |   | new_purchases |   | new_bet    |   | new_win    |\n",
    "|------------------|---|---------------|---|------------|---|------------|\n",
    "| user_id          |   | user_id       |   | user_id    |   | user_id    |\n",
    "| event_type       |   | event_type    |   | event_type |   | event_type |\n",
    "| event_time       |   | event_time    |   | event_time |   | event_time |\n",
    "| name             |   | amount        |   | bet_amount |   | win_amount |\n",
    "| date_of_birth    |   |               |   |            |   |            |\n",
    "| street_address   |   |               |   |            |   |            |\n",
    "| city             |   |               |   |            |   |            |\n",
    "| country          |   |               |   |            |   |            |\n",
    "| postcode         |   |               |   |            |   |            |\n",
    "| affiliate_url    |   |               |   |            |   |            |\n",
    "| campaign         |   |               |   |            |   |            |\n",
    "\n",
    "Furthermore, `new_registration` includes a `label` column to indicate whether or not the user has churned (1 for churned and 0 for not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the faker module to generate data, please run the cell below and restart the notenook's kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already installed: faker\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import IPython\n",
    "\n",
    "required = {'faker'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "previously_installed = required.intersection(installed)\n",
    "\n",
    "if missing:\n",
    "    print(f'Installing {\",\".join(missing)}')\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "    print('Restarting kernel')\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel\n",
    "if previously_installed:\n",
    "    print(f'Already installed: {\",\".join(previously_installed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import load_project\n",
    "from os import path\n",
    "\n",
    "project_path = path.abspath('conf')\n",
    "project = load_project(project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the generated stream path, this is where we output the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container: users\n",
      "Output stream path: iguazio/examples/model-deployment-with-streaming/data/generated-stream\n"
     ]
    }
   ],
   "source": [
    "container = project.params.get('CONTAINER')\n",
    "output_stream = project.params.get('STREAM_CONFIGS').get('generated-stream')\n",
    "output_stream_path =  output_stream.get('path')\n",
    "print(f'Container: {container}\\nOutput stream path: {output_stream_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def gen_postcode(is_churn):\n",
    "    # if is_churn is true the postcode modulu 3 will return 0 or 1\n",
    "    # if is_churn is false the postcode modulu 3 will return 0 or 2\n",
    "    # this will encode information in postcode that our ML model will learn\n",
    "    base_postcode = 3 * randint(3334,33333)\n",
    "    group = randint(0,1)\n",
    "    if is_churn:\n",
    "        return base_postcode + group\n",
    "    else:\n",
    "        return base_postcode + (group * 2)\n",
    "\n",
    "# event functions\n",
    "def new_registration(fake, id, event_time, is_churn):\n",
    "    return {'user_id': id,\n",
    "            'event_type': 'registration',\n",
    "            'event_time': event_time,\n",
    "            'name':fake.name(),\n",
    "            'date_of_birth': fake.date(),\n",
    "            'street_address': fake.street_address(),\n",
    "            'city': fake.city(),\n",
    "            'country': fake.country(),\n",
    "            'postcode': gen_postcode(is_churn),\n",
    "            'affiliate_url': fake.image_url(),\n",
    "            'campaign': fake.ean8()}\n",
    "\n",
    "def new_purchase(fake, id, event_time):\n",
    "    return {'user_id': id,\n",
    "            'event_type': 'purchase',\n",
    "            'event_time': event_time,\n",
    "            'amount': fake.randomize_nb_elements(number=50)}\n",
    "\n",
    "def new_bet(fake, id, event_time):\n",
    "    return {'user_id': id,\n",
    "            'event_type': 'bet',\n",
    "            'event_time': event_time,\n",
    "            'bet_amount': fake.randomize_nb_elements(number=10)}\n",
    "    \n",
    "def new_win(fake, id, event_time):\n",
    "    return {'user_id': id,\n",
    "            'event_type': 'win',\n",
    "            'event_time': event_time,\n",
    "            'win_amount': fake.randomize_nb_elements(number=200)}\n",
    "\n",
    "def gen_event_date(is_churn, prev_event_date=None):\n",
    "    if prev_event_date is None:\n",
    "        #generate first event date\n",
    "        return datetime.now() - timedelta(hours=randint(48,96))\n",
    "    else:\n",
    "        if prev_event_date + timedelta(hours=30) < datetime.now() and not is_churn and randint(1,1000) <= 5:\n",
    "            # if the user is not churned and it is possible, generate event in the following day with prbability 0.005\n",
    "            return prev_event_date + timedelta(hours=randint(15,24))\n",
    "        else:\n",
    "            return prev_event_date + timedelta(seconds=randint(5,100))\n",
    "        \n",
    "def generate_events(fake, user_ids, events_dist, num_events, is_churn):\n",
    "    events = []\n",
    "    for id in user_ids:\n",
    "        # register\n",
    "        event_time = gen_event_date(is_churn)\n",
    "        reg_event = new_registration(fake, id, event_time, is_churn)\n",
    "        reg_event['label'] = int(is_churn)\n",
    "        events.append(reg_event)\n",
    "        for _ in range(num_events):\n",
    "            # generate event according to dist\n",
    "            acc_prob = 0\n",
    "            rand = random()\n",
    "            for event_dist in events_dist:\n",
    "                if rand <= event_dist['probability']+acc_prob:\n",
    "                    event_time = gen_event_date(is_churn, event_time)\n",
    "                    new_event = event_dist['generator'](fake, id, event_time)\n",
    "                    events.append(new_event)\n",
    "                    prob_threshold = 0\n",
    "                    break\n",
    "                else:\n",
    "                    acc_prob += event_dist['probability']\n",
    "    return events\n",
    "\n",
    "# 70% churn users \n",
    "NUM_USERS_GROUP1 = 1400\n",
    "NUM_USERS_GROUP2 = 600 \n",
    "NUM_USERS = NUM_USERS_GROUP1+NUM_USERS_GROUP2\n",
    "\n",
    "EVENTS_PER_USER = 1000\n",
    "\n",
    "GROUP1_EVENTS_DIST = [{'probability': 0.1, 'generator': new_purchase}, \n",
    "                      {'probability': 0.89, 'generator': new_bet}, \n",
    "                      {'probability': 0.01, 'generator': new_win}]\n",
    "\n",
    "GROUP2_EVENTS_DIST = [{'probability': 0.1, 'generator': new_purchase}, \n",
    "                      {'probability': 0.85, 'generator': new_bet},\n",
    "                      {'probability': 0.05, 'generator': new_win}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create V3IO Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataplane client you can manipulate data in the platform's multi-model data layer, including:\n",
    "* Objects\n",
    "* Key-values (NoSQL)\n",
    "* Streams\n",
    "* Containers\n",
    "\n",
    "Under the hood, the client connects through the platform's web API (https://www.iguazio.com/docs/reference/latest-release/api-reference/web-apis/) and wraps each low level API with an interface. Calls are blocking, but you can use the batching interface to send multiple requests in parallel for greater performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import v3io.dataplane\n",
    "from os import getenv\n",
    "v3io_client = v3io.dataplane.Client(endpoint=project.params.get('WEB_API'),\n",
    "                                    access_key=getenv('V3IO_ACCESS_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events generated: 2002000\n",
      "Events preview: [{'user_id': 'bd28cb66-d330-4186-9d43-eb13a6bb2a2d', 'event_type': 'bet', 'event_time': datetime.datetime(2020, 8, 21, 1, 29, 36, 215472), 'bet_amount': 9}, {'user_id': 'bd28cb66-d330-4186-9d43-eb13a6bb2a2d', 'event_type': 'bet', 'event_time': datetime.datetime(2020, 8, 21, 1, 30, 22, 215472), 'bet_amount': 10}, {'user_id': 'bd28cb66-d330-4186-9d43-eb13a6bb2a2d', 'event_type': 'bet', 'event_time': datetime.datetime(2020, 8, 21, 1, 31, 18, 215472), 'bet_amount': 13}, {'user_id': 'bd28cb66-d330-4186-9d43-eb13a6bb2a2d', 'event_type': 'bet', 'event_time': datetime.datetime(2020, 8, 21, 1, 32, 57, 215472), 'bet_amount': 9}]\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "group1_user_ids = (str(uuid.uuid4()) for _ in range(NUM_USERS_GROUP1))\n",
    "group2_user_ids = (str(uuid.uuid4()) for _ in range(NUM_USERS_GROUP2))\n",
    "\n",
    "group1_events = generate_events(fake, group1_user_ids, GROUP1_EVENTS_DIST, EVENTS_PER_USER, True)\n",
    "group2_events = generate_events(fake, group2_user_ids, GROUP2_EVENTS_DIST, EVENTS_PER_USER, False)\n",
    "\n",
    "print(f'Events generated: {len(group1_events)+len(group2_events)}')\n",
    "print(f'Events preview: {group1_events[1:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write generated events to V3IO Steam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort the events based on their event time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = (group1_events + group2_events)\n",
    "events.sort(key=lambda event: event.get('event_time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest in small batches to V3IO Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create enrichment table where the key is postal-code and the value is the socioeconomic index at the area.\n",
    "\n",
    "To get the highest possible throughput, we can send many requests towards the data layer and wait for all the responses to arrive (rather than send each request and wait for the response). The SDK supports this through batching. Any API call can be made through the client's built in `batch` object. The API call receives the exact same arguments it would normally receive (except for `raise_for_status`), and does not block until the response arrives. To wait for all pending responses, call `wait()` on the `batch` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "batch_size = 1000\n",
    "for i in range(0, len(events), batch_size):\n",
    "    # Convert the events to records\n",
    "    records = [{'data': json.dumps(event, default=str)} for event in events[i:i+batch_size]]\n",
    "    v3io_client.batch.put_records(container=container, path=output_stream_path, records=records)\n",
    "\n",
    "responses = v3io_client.batch.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The looped `put_records` interface above will send all `put records` requests to the data layer in parallel. When `wait` is called, it will block until either all responses arrive (in which case it will return a `Responses` object, containing the `responses` of each call) or an error occurs - in which case an exception is thrown. You can pass `raise_for_status` to `wait`, and it behaves as explained above.\n",
    "\n",
    "> Note: The `batch` object is stateful, so you can only create one batch at a time. However, you can create multiple parallel batches yourself through the client's `create_batch()` interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully streamed 2002000\n",
      "All data streamed successfully.\n"
     ]
    }
   ],
   "source": [
    "records_sent = sum(len(json.loads(resp.body)['Records']) for resp in responses)\n",
    "print(f'Records sent {records_sent}')\n",
    "\n",
    "failed_records = sum(json.loads(resp.body)['FailedRecordCount'] for resp in responses)\n",
    "\n",
    "if failed_records > 0:\n",
    "    print(f'Failed to stream {failed_records}')\n",
    "else:\n",
    "    print('All data streamed successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue to [**2-incoming-event-handler.ipynb**](2-incoming-event-handler.ipynb) to process the incoming data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
