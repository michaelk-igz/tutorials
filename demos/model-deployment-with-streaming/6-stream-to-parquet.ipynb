{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Stream to Parquet\n",
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Store the a stream to a set of parquet files. \n",
    "The purpose is to store the input and inference streams to a parquet log.\n",
    "\n",
    "![Model deployment with streaming Real-time operational Pipeline](../../assets/images/model-deployment-with-streaming.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each batch size (default 1024 records) are stored in parquet file partitioned by the event time (year, month, day, hour). </br> \n",
    "The default outputs are to </br> `/User/examples/model-deployment-with-streaming/data/events-pq` </br> `/User/examples/model-deployment-with-streaming/data/inference-pq`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import load_project\n",
    "from os import path\n",
    "\n",
    "project_path = path.abspath('conf')\n",
    "project = load_project(project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Test a Local Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Nuclio](https://nuclio.io/) is a high-performance open-source and managed serverless framework, which is available as a predefined tenant-wide platform service (`nuclio`).\n",
    "The demo uses Nuclio to create and deploy serverless functions.\n",
    "Therefore, you need to import the Nuclio package and configure Nuclio for your project.\n",
    "\n",
    "The platform's Jupyter Notebook service preinstalls the [nuclio-jupyter SDK](https://github.com/nuclio/nuclio-jupyter/blob/master/README.md) for creating and deploying Nuclio functions with Python and Jupyter Notebook.\n",
    "The tutorial uses the Nuclio magic commands and annotation comments of this SDK to automate function code generation.\n",
    "The magic commands are initialized when you import the `nuclio` package.<br>\n",
    "The `%nuclio` magic commands are used to run Nuclio commands from Jupyter notebooks (`%nuclio <Nuclio command>`).\n",
    "You can also use `%%nuclio` at the start of a cell to identify the entire cell as containing Nuclio code.\n",
    "The magic commands are initialized when you import the `nuclio` package.<br>\n",
    "The `# nuclio: start-code`, `# nuclio: end-code`, and `# nuclio: ignore` section-marker annotations notify Nuclio of the beginning or end of code sections.\n",
    "Nuclio ignores all notebook code before a `# nuclio: start-code` marker or after an `# nuclio: end-code` marker.\n",
    "Nuclio translates all other notebook code sections into function code, except for sections that are marked with the `# nuclio: ignore` marker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code imports the `nuclio` Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses the `# nuclio: start-code` marker to instruct Nuclio to start processing code only from this location, and then performs basic Nuclio function configuration &mdash; defining the name of the function's container image (`mlrun/ml-models`), the function type (`nuclio`), and some additional package installation commands.\n",
    "\n",
    "> **Note:** You can add code to define function dependencies and perform additional configuration after the `# nuclio: start-code` marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c\n",
    "\n",
    "python -m pip install pandas\n",
    "python -m pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting spec.build.baseImage to 'mlrun/ml-models'\n",
      "%nuclio: setting spec.readinessTimeoutSeconds to 200\n",
      "%nuclio: setting kind to 'nuclio'\n"
     ]
    }
   ],
   "source": [
    "%%nuclio config\n",
    "spec.build.baseImage = \"mlrun/ml-models\"\n",
    "spec.readinessTimeoutSeconds = 200\n",
    "kind = \"nuclio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    setattr(context, 'batch', [])\n",
    "    setattr(context, 'batch_count', 0)\n",
    "    setattr(context, 'batch_size', int(os.getenv('BATCH_SIZE', 1024)))\n",
    "\n",
    "    setattr(context, 'timestamp_key', os.getenv('TS_KEY'))\n",
    "    setattr(context, 'timestamp_format', os.getenv('TS_FORMAT', '%Y-%m-%d %H:%M:%S.%f'))\n",
    "\n",
    "    setattr(context, 'pq_partitions', ['pq_year', 'pq_month', 'pq_day', 'pq_hour'])\n",
    "\n",
    "    setattr(context, 'target_path', os.getenv('TARGET_PATH'))\n",
    "    os.makedirs(context.target_path, exist_ok=True)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context, event):\n",
    "    if type(event.body) is dict:\n",
    "        event_dict = event.body\n",
    "    else:\n",
    "        event_dict = json.loads(event.body)\n",
    "\n",
    "    context.logger.info_with('Got invoked',\n",
    "                             trigger_kind=event.trigger.kind,\n",
    "                             event_body=event_dict)\n",
    "\n",
    "    event_with_time_partitions = add_time_partition_attributes(context, event_dict)\n",
    "\n",
    "    # add the incoming event to the current batch\n",
    "    context.batch.append(event_with_time_partitions)\n",
    "\n",
    "    # check if batch size reached\n",
    "    if context.batch_size == len(context.batch):\n",
    "        context.logger.info_with('Writing batch',\n",
    "                                 batch_count=context.batch_count,\n",
    "                                 batch_size=len(context.batch))\n",
    "        write_batch(context)\n",
    "        context.logger.info_with('Written batch',\n",
    "                                 batch_count=context.batch_count,\n",
    "                                 batch_size=len(context.batch))\n",
    "    pass\n",
    "\n",
    "\n",
    "def add_time_partition_attributes(context, event):\n",
    "    if hasattr(context, 'timestamp_key') and event.get(context.timestamp_key) is not None:\n",
    "        # parse the event time\n",
    "        dt_object = datetime.strptime(event[context.timestamp_key], context.timestamp_format)\n",
    "    else:\n",
    "        # if event time is missing or not configured, use current datetime\n",
    "        dt_object = datetime.now()\n",
    "\n",
    "    # add the partition attributes\n",
    "    event['pq_year'] = dt_object.strftime('%Y')\n",
    "    event['pq_month'] = dt_object.strftime('%m')\n",
    "    event['pq_day'] = dt_object.strftime('%d')\n",
    "    event['pq_hour'] = dt_object.strftime('%H')\n",
    "\n",
    "    return event\n",
    "\n",
    "\n",
    "def write_batch(context):\n",
    "    df = pd.DataFrame.from_records(context.batch)\n",
    "    df.to_parquet(path=context.target_path, partition_cols=context.pq_partitions)\n",
    "    # post write cleanup\n",
    "    context.batch = []\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell uses the `# nuclio: end-code` marker to mark the end of a Nuclio code section and instruct Nuclio to stop parsing the notebook at this point.<br>\n",
    "> **IMPORTANT:** Do not remove the end-code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set few parameters via environment variables0\n",
    "target_path = path.join(os.sep, 'v3io', project.params.get('CONTAINER'), project.params.get('EVENTS_PARQUET_TARGET_PATH'))\n",
    "envs = {'TARGET_PATH' : target_path,\n",
    "        'BATCH_SIZE': 1024,\n",
    "        'TS_KEY': 'event_time',\n",
    "        'TS_FORMAT': '%Y-%m-%d %H:%M:%S.%f'}\n",
    "\n",
    "for key, value in envs.items():\n",
    "    os.environ[key] = str(value)\n",
    "init_context(context)\n",
    "#reduce the batch size to 10\n",
    "context.batch_size = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python> 2020-10-05 10:05:51,196 [info] Got invoked: {'trigger_kind': '', 'event_body': {'user_id': 1, 'event_type': 'spin', 'event_time': '2020-02-02 12:20:22.333332'}}\n",
      "Python> 2020-10-05 10:05:51,198 [info] Got invoked: {'trigger_kind': '', 'event_body': {'user_id': 2, 'event_type': 'spin', 'event_time': '2020-02-02 12:20:23.333332'}}\n",
      "Python> 2020-10-05 10:05:51,198 [info] Got invoked: {'trigger_kind': '', 'event_body': {'user_id': 3, 'event_type': 'spin', 'event_time': '2020-02-02 12:20:24.333332'}}\n",
      "Python> 2020-10-05 10:05:51,199 [info] Got invoked: {'trigger_kind': '', 'event_body': {'user_id': 4, 'event_type': 'spin', 'event_time': '2020-02-02 12:20:25.333332'}}\n",
      "Python> 2020-10-05 10:05:51,199 [info] Got invoked: {'trigger_kind': '', 'event_body': {'user_id': 5, 'event_type': 'spin', 'event_time': '2020-02-02 12:20:26.333332'}}\n",
      "Python> 2020-10-05 10:05:51,200 [info] Got invoked: {'trigger_kind': '', 'event_body': {'user_id': 6, 'event_type': 'spin', 'event_time': '2020-02-02 12:20:27.333332'}}\n",
      "Python> 2020-10-05 10:05:51,200 [info] Got invoked: {'trigger_kind': '', 'event_body': {'user_id': 7, 'event_type': 'spin', 'event_time': '2020-02-02 12:20:28.333332'}}\n",
      "Python> 2020-10-05 10:05:51,201 [info] Got invoked: {'trigger_kind': '', 'event_body': {'user_id': 8, 'event_type': 'spin', 'event_time': '2020-02-02 12:20:29.333332'}}\n",
      "Python> 2020-10-05 10:05:51,201 [info] Got invoked: {'trigger_kind': '', 'event_body': {'user_id': 9, 'event_type': 'spin', 'event_time': '2020-02-02 12:20:30.333332'}}\n"
     ]
    }
   ],
   "source": [
    "# trigger with 9 events:\n",
    "\n",
    "nine_events = [b'{\"user_id\" : 1 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:22.333332\"}',\n",
    "              b'{\"user_id\" : 2 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:23.333332\"}',\n",
    "              b'{\"user_id\" : 3 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:24.333332\"}',\n",
    "              b'{\"user_id\" : 4 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:25.333332\"}',\n",
    "              b'{\"user_id\" : 5 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:26.333332\"}',\n",
    "              b'{\"user_id\" : 6 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:27.333332\"}',\n",
    "              b'{\"user_id\" : 7 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:28.333332\"}',\n",
    "              b'{\"user_id\" : 8 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:29.333332\"}',\n",
    "              b'{\"user_id\" : 9 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:30.333332\"}']\n",
    "\n",
    "for e in nine_events:\n",
    "    event = nuclio.Event(body=e)\n",
    "    handler(context, event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/User/examples/model-deployment-with-streaming/data/events-pq': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# check whether a parquet has been created\n",
    "!ls -l {target_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python> 2020-10-05 10:06:39,411 [info] Got invoked: {'trigger_kind': '', 'event_body': {'user_id': 10, 'event_type': 'spin', 'event_time': '2020-02-02 12:20:31.333332'}}\n",
      "Python> 2020-10-05 10:06:39,412 [info] Writing batch: {'batch_count': 0, 'batch_size': 10}\n",
      "Python> 2020-10-05 10:06:39,500 [info] Written batch: {'batch_count': 0, 'batch_size': 0}\n"
     ]
    }
   ],
   "source": [
    "# trigger the tenth event which should trigger the creation of the parquet file.\n",
    "tenth_event = b'{\"user_id\" : 10 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:31.333332\"}'\n",
    "event = nuclio.Event(body=tenth_event)\n",
    "handler(context, event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxr-xr-x 2 51 nogroup 0 Oct  5 10:06 'pq_year=2020'\n"
     ]
    }
   ],
   "source": [
    "# check weather a parquet has been created\n",
    "!ls -l {target_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "!rm -rf {target_path}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuclio Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuclio leverages consumer groups. When one or more Nuclio replicas join a consumer group, each replica receives its equal share of the shards, based on the number of replicas that are defined in the function.\n",
    "\n",
    "We set up the input stream URL below. A consumer-group URL is in the form of `http://v3io-webapi:8081/<container name>/<stream path>@<consumer group name>`. In this case we use `WEB_API_USERS` for URL prefix `http://v3io-webapi:8081/<container name>` and a consumer group named **`stream2pq`**.\n",
    "\n",
    "For more information, refer to the [Nuclio v3iostream trigger reference documentation](https://nuclio.io/docs/latest/reference/triggers/v3iostream/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input incoming-events-stream path: michaelk/examples/model-deployment-with-streaming/data/incoming-events-stream\n",
      "Input incoming-events-stream URL: http://v3io-webapi:8081/users/michaelk/examples/model-deployment-with-streaming/data/incoming-events-stream@stream2pq\n",
      "Input inference-stream path: michaelk/examples/model-deployment-with-streaming/data/inference-stream\n",
      "Input inference-stream URL: http://v3io-webapi:8081/users/michaelk/examples/model-deployment-with-streaming/data/inference-stream@stream2pq\n"
     ]
    }
   ],
   "source": [
    "def get_stream_url(name):\n",
    "    input_stream = project.params.get('STREAM_CONFIGS').get(name)\n",
    "    input_stream_path =  input_stream.get('path')\n",
    "    print(f'Input {name} path: {input_stream_path}')\n",
    "    WEB_API_USERS = project.params.get('WEB_API_USERS')\n",
    "    input_stream_url = path.join(WEB_API_USERS, input_stream_path) + \"@stream2pq\"\n",
    "    print(f'Input {name} URL: {input_stream_url}')\n",
    "    return input_stream_url\n",
    "\n",
    "events_stream_url = get_stream_url('incoming-events-stream')\n",
    "inference_stream_url = get_stream_url('inference-stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `target_path`s for the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events target path: /User/examples/model-deployment-with-streaming/data/events-pq\n",
      "Inference target path: /User/examples/model-deployment-with-streaming/data/inference-pq\n"
     ]
    }
   ],
   "source": [
    "events_target_path = path.join(os.sep, 'v3io', project.params.get('CONTAINER'), project.params.get('EVENTS_PARQUET_TARGET_PATH'))\n",
    "inference_target_path = path.join(os.sep, 'v3io', project.params.get('CONTAINER'), project.params.get('INFERENCE_PARQUET_TARGET_PATH'))\n",
    "print(f'Events target path: {events_target_path}\\nInference target path: {inference_target_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a dictionary for initializing the environment variables used by each of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_envs = {'TARGET_PATH' : events_target_path,\n",
    "        'BATCH_SIZE': 1024,\n",
    "        'TS_KEY': 'event_time',\n",
    "        'TS_FORMAT': '%Y-%m-%d %H:%M:%S.%f'}\n",
    "\n",
    "inference_envs = {'TARGET_PATH' : inference_target_path,\n",
    "        'BATCH_SIZE': 1024,\n",
    "        'TS_KEY': 'when',\n",
    "        'TS_FORMAT': '%Y-%m-%d %H:%M:%S.%f'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert code to function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use MLRun `code_to_function` in order to convert the python code to a Nuclio function. We then set the relevant enrivonment variables and streaming trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f195e8b3350>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun import code_to_function, mount_v3io\n",
    "\n",
    "gen_func = code_to_function(name='stream2pq', kind = 'nuclio')\n",
    "project.set_function(gen_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure function instances\n",
    "Here we configure a function instances for each of the streams we want to use `stream to parquet` upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_stream2pq_function(name, envs, input_stream_url):\n",
    "    stream2pq = project.func('stream2pq').copy()\n",
    "    stream2pq.metadata.name = name\n",
    "    stream2pq.set_envs(envs)\n",
    "    stream2pq.add_trigger('stream2pq',\n",
    "                          nuclio.triggers.V3IOStreamTrigger(url=input_stream_url,\n",
    "                                                            access_key=os.getenv('V3IO_ACCESS_KEY'),\n",
    "                                                            maxWorkers=10))\n",
    "    # Configure a mount on the nuclio function from '/User' to our home directory '~/'.\n",
    "    stream2pq.apply(mount_v3io())\n",
    "    return stream2pq\n",
    "\n",
    "events_s2pq = configure_stream2pq_function('events-s2pq', events_envs, events_stream_url)\n",
    "inference_s2pq = configure_stream2pq_function('inference-s2pq', inference_envs, inference_stream_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-10-05 10:08:13,238 [info] deploy started\n",
      "[nuclio] 2020-10-05 10:08:15,342 (info) Build complete\n",
      "[nuclio] 2020-10-05 10:10:34,736 (info) Function deploy complete\n",
      "[nuclio] 2020-10-05 10:10:34,742 done updating model-deployment-with-streaming-michaelk-events-s2pq, function address: 192.168.226.12:31566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://192.168.226.12:31566'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_s2pq.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-10-05 10:11:15,842 [info] deploy started\n",
      "[nuclio] 2020-10-05 10:11:15,922 (info) Building processor image\n",
      "[nuclio] 2020-10-05 10:11:17,943 (info) Build complete\n",
      "[nuclio] 2020-10-05 10:13:33,287 (info) Function deploy complete\n",
      "[nuclio] 2020-10-05 10:13:33,293 done updating model-deployment-with-streaming-michaelk-inference-s2pq, function address: 192.168.226.12:32270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://192.168.226.12:32270'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_s2pq.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
